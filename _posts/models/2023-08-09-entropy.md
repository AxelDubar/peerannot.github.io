---
layout: post
title: Entropy
category: [doc]
tags: [models]
permalink: /models/entropy/
css:
  badge: true
  syntax: true
  custom: >-
    table { font-size: .95rem; margin-bottom: 1.5rem; }
    tr:nth-child(odd) { backgroud-color: #e3edf3; }
    th, td { padding: .5em; }
    .box { border: 1px solid #e3edf3; padding: 1rem; }
    .plaintext { color: mediumseagreen; }
excerpt: Models available in peerannot
featured: false

---

## Identification step

The entropy identification measures how diverse the labels answered to a given task are.
With enough labels, tasks with high entropy can be identified as potential ambiguities/difficulties.
A task with a null-entropy has reached perfect consensus.

More formally, let task $x_i\in\mathcal{X}$ for a classification problem with $K$ possible labels.
From the multiple answers, we can compute the frequency of each answers per label denoted $\hat{y\_i^{\mathrm{NS}}} = \mathrm{NS}(\{y\_i^{(j)}\}_{j\in [n\_\text{worker}]})$.

$$
\forall x_i,\ \mathrm{Ent}(x_i) = -\sum_{k\in[K]} \hat{y_{i, k}^{\mathrm{NS}}} \log\left(\hat{y_{i, k}^{\mathrm{NS}}}\right) \enspace.
$$


## CLI

For a classification problem, with $K=10$, if the json file of answers is located at the current directory simply run:

```bash
peerannot identify . -s entropy -K 10
```

This creates a file named `entropy.npy` saved in the `./identification` folder.
